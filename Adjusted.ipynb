{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43a6734e-dddf-40bb-a1b7-2c0644deb145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  3 of 3 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating models for S&P 500...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0376 - mae: 0.1309\n",
      "Epoch 2/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0032 - mae: 0.0426\n",
      "Epoch 3/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0030 - mae: 0.0412\n",
      "Epoch 4/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0367\n",
      "Epoch 5/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0364\n",
      "Epoch 6/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0022 - mae: 0.0344\n",
      "Epoch 7/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - mae: 0.0352\n",
      "Epoch 8/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - mae: 0.0352\n",
      "Epoch 9/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0021 - mae: 0.0340\n",
      "Epoch 10/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0021 - mae: 0.0345\n",
      "Epoch 11/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0018 - mae: 0.0314\n",
      "Epoch 12/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0017 - mae: 0.0303\n",
      "Epoch 13/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0294\n",
      "Epoch 14/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0020 - mae: 0.0321\n",
      "Epoch 15/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0309\n",
      "Epoch 16/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0020 - mae: 0.0320\n",
      "Epoch 17/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0287\n",
      "Epoch 18/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0289\n",
      "Epoch 19/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0286\n",
      "Epoch 20/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0260\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training and evaluating models for NASDAQ...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0474 - mae: 0.1520\n",
      "Epoch 2/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0032 - mae: 0.0421\n",
      "Epoch 3/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0026 - mae: 0.0374\n",
      "Epoch 4/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0029 - mae: 0.0395\n",
      "Epoch 5/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0022 - mae: 0.0349\n",
      "Epoch 6/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0354\n",
      "Epoch 7/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - mae: 0.0349\n",
      "Epoch 8/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0019 - mae: 0.0327\n",
      "Epoch 9/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0019 - mae: 0.0320\n",
      "Epoch 10/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0019 - mae: 0.0315\n",
      "Epoch 11/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0307\n",
      "Epoch 12/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0017 - mae: 0.0298\n",
      "Epoch 13/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0299\n",
      "Epoch 14/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0299\n",
      "Epoch 15/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0282\n",
      "Epoch 16/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0017 - mae: 0.0286\n",
      "Epoch 17/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0268\n",
      "Epoch 18/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0017 - mae: 0.0301\n",
      "Epoch 19/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0289\n",
      "Epoch 20/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0256\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training and evaluating models for Nikkei 225...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0596 - mae: 0.1603\n",
      "Epoch 2/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0510\n",
      "Epoch 3/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0036 - mae: 0.0439\n",
      "Epoch 4/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0033 - mae: 0.0420\n",
      "Epoch 5/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0027 - mae: 0.0384\n",
      "Epoch 6/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0032 - mae: 0.0415\n",
      "Epoch 7/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0028 - mae: 0.0381\n",
      "Epoch 8/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0364\n",
      "Epoch 9/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0357\n",
      "Epoch 10/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0349\n",
      "Epoch 11/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - mae: 0.0340\n",
      "Epoch 12/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - mae: 0.0345\n",
      "Epoch 13/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0020 - mae: 0.0321\n",
      "Epoch 14/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0021 - mae: 0.0329\n",
      "Epoch 15/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - mae: 0.0343\n",
      "Epoch 16/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0019 - mae: 0.0321\n",
      "Epoch 17/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0020 - mae: 0.0318\n",
      "Epoch 18/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0019 - mae: 0.0320\n",
      "Epoch 19/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0020 - mae: 0.0318\n",
      "Epoch 20/20\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0303\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Final Results for LSTM:\n",
      "S&P 500 - MSE: 0.06622, MAE: 0.19091, R^2: 0.55143\n",
      "NASDAQ - MSE: 0.14691, MAE: 0.25791, R^2: 0.57713\n",
      "Nikkei 225 - MSE: 0.00420, MAE: 0.04580, R^2: 0.91283\n",
      "\n",
      "Final Results for SVM:\n",
      "S&P 500 - MSE: 0.81760, MAE: 0.72341, R^2: -4.53847\n",
      "NASDAQ - MSE: 1.64011, MAE: 1.05322, R^2: -3.72077\n",
      "Nikkei 225 - MSE: 0.15660, MAE: 0.25407, R^2: -2.25152\n",
      "\n",
      "Final Results for XGBoost:\n",
      "S&P 500 - MSE: 0.29393, MAE: 0.39255, R^2: -0.99110\n",
      "NASDAQ - MSE: 0.75813, MAE: 0.64359, R^2: -1.18215\n",
      "Nikkei 225 - MSE: 0.04868, MAE: 0.14020, R^2: -0.01084\n",
      "\n",
      "Final Results for LinearRegression:\n",
      "S&P 500 - MSE: 0.00120, MAE: 0.02158, R^2: 0.99189\n",
      "NASDAQ - MSE: 0.00115, MAE: 0.02354, R^2: 0.99669\n",
      "Nikkei 225 - MSE: 0.00041, MAE: 0.01510, R^2: 0.99139\n",
      "\n",
      "Final Results for MLP:\n",
      "S&P 500 - MSE: 0.00656, MAE: 0.06211, R^2: 0.95558\n",
      "NASDAQ - MSE: 0.01828, MAE: 0.09825, R^2: 0.94738\n",
      "Nikkei 225 - MSE: 0.00193, MAE: 0.03213, R^2: 0.95998\n",
      "\n",
      "Final Results for RandomForest:\n",
      "S&P 500 - MSE: 0.29260, MAE: 0.38957, R^2: -0.98207\n",
      "NASDAQ - MSE: 0.75991, MAE: 0.64552, R^2: -1.18726\n",
      "Nikkei 225 - MSE: 0.04988, MAE: 0.14201, R^2: -0.03560\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "tickers = ['^GSPC', '^IXIC', '^N225']\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2022-01-01'\n",
    "train_end_date = '2018-01-01'  # Split date\n",
    "\n",
    "n_steps = 30\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "selected_features = ['Open', 'High', 'Low', 'Adj Close', 'Volume', 'RSI_14', 'MACD', 'Signal_Line']\n",
    "\n",
    "# -----------------------------\n",
    "# Data Fetching\n",
    "# -----------------------------\n",
    "data = yf.download(tickers, start=start_date, end=end_date, group_by='ticker')\n",
    "\n",
    "# Remove 'Close' columns to avoid confusion (already have Adj Close)\n",
    "for ticker in tickers:\n",
    "    if (ticker, 'Close') in data.columns:\n",
    "        data.drop(columns=[(ticker, 'Close')], inplace=True)\n",
    "\n",
    "# Forward-fill missing values\n",
    "data = data.ffill()\n",
    "\n",
    "# -----------------------------\n",
    "# Compute Technical Indicators\n",
    "# -----------------------------\n",
    "window_rsi = 14\n",
    "window_macd_short = 12\n",
    "window_macd_long = 26\n",
    "signal_line_window = 9\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Compute RSI\n",
    "    delta = data[(ticker, 'Adj Close')].diff(1)\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=window_rsi).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(window=window_rsi).mean()\n",
    "    RS = gain / loss\n",
    "    rsi_series = 100 - (100 / (1 + RS))\n",
    "\n",
    "    # Insert RSI after Adj Close column\n",
    "    adj_close_col_pos = data.columns.get_loc((ticker, 'Adj Close')) + 1\n",
    "    data.insert(loc=adj_close_col_pos, column=(ticker, 'RSI_14'), value=rsi_series)\n",
    "\n",
    "    # Compute MACD\n",
    "    exp1 = data[(ticker, 'Adj Close')].ewm(span=window_macd_short, adjust=False).mean()\n",
    "    exp2 = data[(ticker, 'Adj Close')].ewm(span=window_macd_long, adjust=False).mean()\n",
    "    macd = exp1 - exp2\n",
    "    signal_line = macd.ewm(span=signal_line_window, adjust=False).mean()\n",
    "\n",
    "    # Insert MACD and Signal Line after RSI column\n",
    "    rsi_col_pos = data.columns.get_loc((ticker, 'RSI_14')) + 1\n",
    "    data.insert(loc=rsi_col_pos, column=(ticker, 'MACD'), value=macd)\n",
    "    data.insert(loc=rsi_col_pos + 1, column=(ticker, 'Signal_Line'), value=signal_line)\n",
    "\n",
    "# Drop rows with NaN (due to RSI/MACD calculations)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Train/Test Split Before Scaling\n",
    "# -----------------------------\n",
    "train_data = data[data.index < train_end_date]\n",
    "test_data = data[data.index >= train_end_date]\n",
    "\n",
    "# We'll scale each ticker's features separately for clarity.\n",
    "# Store scalers so we can apply the same scaler to test data.\n",
    "scalers = {}\n",
    "\n",
    "def scale_ticker_data(train_df, test_df, ticker):\n",
    "    # Extract the features for the current ticker\n",
    "    ticker_features = [(ticker, f) for f in selected_features if (ticker, f) in train_df.columns]\n",
    "\n",
    "    if not ticker_features:\n",
    "        return train_df, test_df  # In case some data is missing\n",
    "\n",
    "    # Fit scaler on training data only\n",
    "    scaler = MinMaxScaler()\n",
    "    train_values = train_df.loc[:, ticker_features].values\n",
    "    train_scaled = scaler.fit_transform(train_values)\n",
    "\n",
    "    # Apply to test data\n",
    "    test_values = test_df.loc[:, ticker_features].values\n",
    "    test_scaled = scaler.transform(test_values)\n",
    "\n",
    "    # Replace original columns with scaled values\n",
    "    train_df.loc[:, ticker_features] = train_scaled\n",
    "    test_df.loc[:, ticker_features] = test_scaled\n",
    "\n",
    "    scalers[ticker] = scaler\n",
    "    return train_df, test_df\n",
    "\n",
    "# Scale data ticker by ticker\n",
    "for ticker in tickers:\n",
    "    train_data, test_data = scale_ticker_data(train_data, test_data, ticker)\n",
    "\n",
    "# Separate DataFrames for each ticker after scaling\n",
    "gspc_df = pd.concat([train_data['^GSPC'], test_data['^GSPC']])\n",
    "ixic_df = pd.concat([train_data['^IXIC'], test_data['^IXIC']])\n",
    "n225_df = pd.concat([train_data['^N225'], test_data['^N225']])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Sequence Creation Functions\n",
    "# -----------------------------\n",
    "def create_lstm_sequences(df, target_col, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - n_steps):\n",
    "        X.append(df.iloc[i:i+n_steps][selected_features].values)\n",
    "        y.append(df.iloc[i+n_steps][target_col])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_sequences_sklearn(df, target_col, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - n_steps):\n",
    "        seq_features = df.iloc[i:i+n_steps][selected_features].values.flatten()\n",
    "        X.append(seq_features)\n",
    "        y.append(df.iloc[i+n_steps][target_col])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='tanh', return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, activation='tanh'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Training and Evaluation\n",
    "# -----------------------------\n",
    "results = {\"LSTM\": {}, \"SVM\": {}, \"XGBoost\": {}, \"LinearRegression\": {}, \"MLP\": {}, \"RandomForest\": {}}\n",
    "\n",
    "indices_dict = {\n",
    "    \"S&P 500\": gspc_df,\n",
    "    \"NASDAQ\": ixic_df,\n",
    "    \"Nikkei 225\": n225_df\n",
    "}\n",
    "\n",
    "for index_name, df in indices_dict.items():\n",
    "    print(f\"\\nTraining and evaluating models for {index_name}...\")\n",
    "\n",
    "    # Split back into train/test using the chosen date\n",
    "    train_idx = df.index < train_end_date\n",
    "    test_idx = df.index >= train_end_date\n",
    "    train_subset = df[train_idx].copy()\n",
    "    test_subset = df[test_idx].copy()\n",
    "\n",
    "    # LSTM\n",
    "    X_train_lstm, y_train_lstm = create_lstm_sequences(train_subset, target_col='Adj Close', n_steps=n_steps)\n",
    "    X_test_lstm, y_test_lstm = create_lstm_sequences(test_subset, target_col='Adj Close', n_steps=n_steps)\n",
    "    if X_train_lstm.size == 0 or X_test_lstm.size == 0:\n",
    "        print(f\"Not enough data for LSTM for {index_name}, skipping.\")\n",
    "        continue\n",
    "    lstm_model = create_lstm_model(input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]))\n",
    "    lstm_model.fit(X_train_lstm, y_train_lstm, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    y_pred_lstm = lstm_model.predict(X_test_lstm).flatten()\n",
    "    results[\"LSTM\"][index_name] = {\n",
    "        \"MSE\": mean_squared_error(y_test_lstm, y_pred_lstm),\n",
    "        \"MAE\": mean_absolute_error(y_test_lstm, y_pred_lstm),\n",
    "        \"R^2\": r2_score(y_test_lstm, y_pred_lstm),\n",
    "    }\n",
    "\n",
    "    # SVM\n",
    "    X_train_svm, y_train_svm = create_sequences_sklearn(train_subset, target_col='Adj Close', n_steps=n_steps)\n",
    "    X_test_svm, y_test_svm = create_sequences_sklearn(test_subset, target_col='Adj Close', n_steps=n_steps)\n",
    "    if X_train_svm.size == 0 or X_test_svm.size == 0:\n",
    "        print(f\"Not enough data for SVM for {index_name}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # No double scaling here; data already scaled\n",
    "    svm_model = SVR(kernel='rbf', C=100, epsilon=0.01)\n",
    "    svm_model.fit(X_train_svm, y_train_svm)\n",
    "    y_pred_svm = svm_model.predict(X_test_svm)\n",
    "    results[\"SVM\"][index_name] = {\n",
    "        \"MSE\": mean_squared_error(y_test_svm, y_pred_svm),\n",
    "        \"MAE\": mean_absolute_error(y_test_svm, y_pred_svm),\n",
    "        \"R^2\": r2_score(y_test_svm, y_pred_svm),\n",
    "    }\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    xgb_model.fit(X_train_svm, y_train_svm)\n",
    "    y_pred_xgb = xgb_model.predict(X_test_svm)\n",
    "    results[\"XGBoost\"][index_name] = {\n",
    "        \"MSE\": mean_squared_error(y_test_svm, y_pred_xgb),\n",
    "        \"MAE\": mean_absolute_error(y_test_svm, y_pred_xgb),\n",
    "        \"R^2\": r2_score(y_test_svm, y_pred_xgb),\n",
    "    }\n",
    "\n",
    "    # Linear Regression\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_svm, y_train_svm)\n",
    "    y_pred_lr = lr_model.predict(X_test_svm)\n",
    "    results[\"LinearRegression\"][index_name] = {\n",
    "        \"MSE\": mean_squared_error(y_test_svm, y_pred_lr),\n",
    "        \"MAE\": mean_absolute_error(y_test_svm, y_pred_lr),\n",
    "        \"R^2\": r2_score(y_test_svm, y_pred_lr),\n",
    "    }\n",
    "\n",
    "    # MLP Regressor\n",
    "    mlp_model = MLPRegressor(hidden_layer_sizes=(100,50), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "    mlp_model.fit(X_train_svm, y_train_svm)\n",
    "    y_pred_mlp = mlp_model.predict(X_test_svm)\n",
    "    results[\"MLP\"][index_name] = {\n",
    "        \"MSE\": mean_squared_error(y_test_svm, y_pred_mlp),\n",
    "        \"MAE\": mean_absolute_error(y_test_svm, y_pred_mlp),\n",
    "        \"R^2\": r2_score(y_test_svm, y_pred_mlp),\n",
    "    }\n",
    "\n",
    "    # Random Forest Regressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, max_depth=None, random_state=42)\n",
    "    rf_model.fit(X_train_svm, y_train_svm)\n",
    "    y_pred_rf = rf_model.predict(X_test_svm)\n",
    "    results[\"RandomForest\"][index_name] = {\n",
    "        \"MSE\": mean_squared_error(y_test_svm, y_pred_rf),\n",
    "        \"MAE\": mean_absolute_error(y_test_svm, y_pred_rf),\n",
    "        \"R^2\": r2_score(y_test_svm, y_pred_rf),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Print Final Results\n",
    "# -----------------------------\n",
    "for model_name, model_results in results.items():\n",
    "    print(f\"\\nFinal Results for {model_name}:\")\n",
    "    for index_name, metrics in model_results.items():\n",
    "        print(f\"{index_name} - MSE: {metrics['MSE']:.5f}, MAE: {metrics['MAE']:.5f}, R^2: {metrics['R^2']:.5f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528d61b-6473-44af-90c6-3cd4041e62dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
